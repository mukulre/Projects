{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9NWYDwhMv/dSm9MpJZ/7y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukulre/Projects/blob/main/Language_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e3i9ykk1vpLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0db27aea-9b95-432f-82ba-e1f1357f1297"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "\n",
        "import os\n",
        "import string\n",
        "from string import digits\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Input, LSTM, Embedding, Dense\n",
        "from keras.models import Model\n",
        "\n",
        "lines=pd.read_csv(\"Hindi_English_Truncated_Corpus.csv\",encoding='utf-8')\n",
        "lines=lines[lines['source']=='ted']\n",
        "lines=lines[~pd.isnull(lines['english_sentence'])]\n",
        "lines.drop_duplicates(inplace=True)\n",
        "# Let us pick any 25000 rows from the dataset\n",
        "lines=lines.sample(n=25000,random_state=42)\n",
        "lines.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.lower())\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.lower())"
      ],
      "metadata": {
        "id": "lW56cDSSvtsD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\"'\", '', x))\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\"'\", '', x))"
      ],
      "metadata": {
        "id": "Gimh_yU4vtuo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exclude = set(string.punctuation) # Set of all special characters\n",
        "# Remove all the special characters\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: ''.join(ch for ch in x if ch not in exclude))"
      ],
      "metadata": {
        "id": "5-S95zyvvtw3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_digits = str.maketrans('', '', digits)\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.translate(remove_digits))\n",
        "\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x: re.sub(\"[२३०८१५७९४६]\", \"\", x))\n",
        "\n",
        "# Remove extra spaces\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: x.strip())\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: x.strip())\n",
        "lines['english_sentence']=lines['english_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "lines['hindi_sentence']=lines['hindi_sentence'].apply(lambda x: re.sub(\" +\", \" \", x))\n",
        "\n",
        "lines['hindi_sentence'] = lines['hindi_sentence'].apply(lambda x : 'START_ '+ x + ' _END')"
      ],
      "metadata": {
        "id": "W_OdbsZ_vtzZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Get English and Hindi Vocabulary\n",
        "all_eng_words=set()\n",
        "for eng in lines['english_sentence']:\n",
        "    for word in eng.split():\n",
        "        if word not in all_eng_words:\n",
        "            all_eng_words.add(word)\n",
        "\n",
        "all_hindi_words=set()\n",
        "for hin in lines['hindi_sentence']:\n",
        "    for word in hin.split():\n",
        "        if word not in all_hindi_words:\n",
        "            all_hindi_words.add(word)\n",
        "lines['length_eng_sentence']=lines['english_sentence'].apply(lambda x:len(x.split(\" \")))\n",
        "lines['length_hin_sentence']=lines['hindi_sentence'].apply(lambda x:len(x.split(\" \")))"
      ],
      "metadata": {
        "id": "4XcOJEDGyNmA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines=lines[lines['length_eng_sentence']<=20]\n",
        "lines=lines[lines['length_hin_sentence']<=20]\n",
        "max_length_src=max(lines['length_hin_sentence'])\n",
        "max_length_tar=max(lines['length_eng_sentence'])\n",
        "\n",
        "input_words = sorted(list(all_eng_words))\n",
        "target_words = sorted(list(all_hindi_words))\n",
        "num_encoder_tokens = len(all_eng_words)\n",
        "num_decoder_tokens = len(all_hindi_words)\n",
        "num_encoder_tokens, num_decoder_tokens\n",
        "\n",
        "num_decoder_tokens += 1 #for zero padding\n",
        "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
        "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
        "reverse_input_char_index = dict((i, word) for word, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, word) for word, i in target_token_index.items())\n",
        "lines = shuffle(lines)"
      ],
      "metadata": {
        "id": "dqcFrgc4yNqf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = lines['english_sentence'], lines['hindi_sentence']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state=42)\n",
        "\n",
        "X_train.to_pickle('X_train.pkl')\n",
        "X_test.to_pickle('X_test.pkl')"
      ],
      "metadata": {
        "id": "1wjdaKo7vt1q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf  # Import tensorflow\n",
        "import numpy as np  # Import numpy\n",
        "\n",
        "X_train = pd.read_pickle('X_train.pkl')\n",
        "X_test = pd.read_pickle('X_test.pkl')\n",
        "\n",
        "# Define missing variables (Ensure you replace with actual values)\n",
        "batch_size = 128\n",
        "max_length_src = 50  # Example max length of source sequences\n",
        "max_length_tar = 50  # Example max length of target sequences\n",
        "num_decoder_tokens = 1000  # Example vocabulary size\n",
        "epochs = 10\n",
        "train_samples = 10000  # Example training samples\n",
        "val_samples = 2000  # Example validation samples\n",
        "\n",
        "# Example dictionaries (Ensure they contain actual mappings)\n",
        "input_token_index = {\"example\": 1}  # Replace with real token mappings\n",
        "target_token_index = {\"example\": 1, \"START_\": 2, \"END_\": 3}  # Replace with real token mappings\n",
        "\n",
        "# Placeholder data (Replace with actual training/testing data)\n",
        "X_train = [\"example sentence\"] * train_samples\n",
        "y_train = [\"START_ example END_\"] * train_samples\n",
        "X_test = [\"example test\"] * val_samples\n",
        "y_test = [\"START_ test END_\"] * val_samples\n",
        "\n",
        "def generate_batch(X, y, batch_size=128):\n",
        "    ''' Generate a batch of data '''\n",
        "    while True:\n",
        "        for j in range(0, len(X), batch_size):\n",
        "\n",
        "            encoder_input_data = np.zeros((actual_batch_size, max_length_src), dtype='float32')\n",
        "            decoder_input_data = np.zeros((actual_batch_size, max_length_tar), dtype='float32')\n",
        "            decoder_target_data = np.zeros((actual_batch_size, max_length_tar, num_decoder_tokens), dtype='float32')\n",
        "\n",
        "            actual_batch_size = min(batch_size, len(X) - j)  # Handle last batch\n",
        "            for i, (input_text, target_text) in enumerate(zip(X[j:j + actual_batch_size], y[j:j + actual_batch_size])):\n",
        "                for t, word in enumerate(input_text.split()):\n",
        "                        encoder_input_data[i, t] = input_token_index[word]  # encoder input seq\n",
        "                for t, word in enumerate(target_text.split()):\n",
        "                    if word in target_token_index:\n",
        "                        if t < len(target_text.split()) - 1:\n",
        "                            decoder_input_data[i, t] = target_token_index[word]  # decoder input seq\n",
        "                        if t > 0:\n",
        "                            decoder_target_data[i, t - 1, target_token_index[word]] = 1.  # one-hot encoding\n",
        "\n",
        "            yield ([encoder_input_data, decoder_input_data], decoder_target_data)\n",
        "\n",
        "# Define output signature for the generator\n",
        "output_signature = (\n",
        "    (tf.TensorSpec(shape=(None, max_length_src), dtype=tf.float32),\n",
        "     tf.TensorSpec(shape=(None, max_length_tar), dtype=tf.float32)),\n",
        "    tf.TensorSpec(shape=(None, max_length_tar, num_decoder_tokens), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "# Convert the generator to a tf.data.Dataset and specify output_signature\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batch(X_train, y_train, batch_size),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batch(X_test, y_test, batch_size),\n",
        "    output_signature=output_signature\n",
        ")\n",
        "\n",
        "# Dummy Model (Ensure this matches your real model)\n",
        "encoder_inputs = tf.keras.Input(shape=(max_length_src,))\n",
        "decoder_inputs = tf.keras.Input(shape=(max_length_tar,))\n",
        "reshaped_decoder_inputs = tf.keras.layers.Reshape((max_length_tar,1))(decoder_inputs)\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    num_decoder_tokens,\n",
        "    output_dim=256,\n",
        "    #embedding_dim=256  # Choose an appropriate embedding dimension\n",
        ")\n",
        "embedded_decoder_inputs = embedding_layer(decoder_inputs)\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "lstm_layer = tf.keras.layers.LSTM(512, return_sequences=True)  # Choose an appropriate LSTM unit size\n",
        "lstm_output = lstm_layer(embedded_decoder_inputs)\n",
        "decoder_outputs = tf.keras.layers.Dense(num_decoder_tokens, activation='softmax')(lstm_output)\n",
        "\n",
        "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Fit the model using the tf.data.Dataset objects\n",
        "model.fit(train_dataset,\n",
        "          steps_per_epoch=train_samples // batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=val_dataset,\n",
        "          validation_steps=val_samples // batch_size)\n",
        "\n",
        "k = 0  # Initialize k to 0 before incrementing\n",
        "k += 1\n",
        "(input_seq, actual_output), _ = next(train_gen)\n",
        "decoded_sentence = decode_sequence(input_seq)\n",
        "print('Input English sentence:', X_train[k:k + 1].values[0])\n",
        "print('Actual Hindi Translation:', y_train[k:k + 1].values[0][6:-4])\n",
        "print('Predicted Hindi Translation:', decoded_sentence[:-4])"
      ],
      "metadata": {
        "id": "7sOArVItvt5Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 877
        },
        "outputId": "e17034cd-e87b-403b-d922-0455a37597cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnboundLocalError: cannot access local variable 'actual_batch_size' where it is not associated with a value\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-25-77ee098ad310>\", line 31, in generate_batch\n    encoder_input_data = np.zeros((actual_batch_size, max_length_src), dtype='float32')\n                                   ^^^^^^^^^^^^^^^^^\n\nUnboundLocalError: cannot access local variable 'actual_batch_size' where it is not associated with a value\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_19064]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-77ee098ad310>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;31m# Fit the model using the tf.data.Dataset objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m model.fit(train_dataset,\n\u001b[0m\u001b[1;32m     87\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nUnboundLocalError: cannot access local variable 'actual_batch_size' where it is not associated with a value\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-25-77ee098ad310>\", line 31, in generate_batch\n    encoder_input_data = np.zeros((actual_batch_size, max_length_src), dtype='float32')\n                                   ^^^^^^^^^^^^^^^^^\n\nUnboundLocalError: cannot access local variable 'actual_batch_size' where it is not associated with a value\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_19064]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6OrZSqrtygBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-CgLGBzyf-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bVddObKGyf8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jAOO3lMyf5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RPE9F1zDyf3Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}